{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76be9b8b",
   "metadata": {},
   "source": [
    "# ENERJİSA VERİ BİLİMİ BOOTCAMP HAFTA 5 ÖDEVİ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfd5db9",
   "metadata": {},
   "source": [
    "### SORU 1:\n",
    "One hot encoding modelin görmediği veriye nasıl uygulanır?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d4ce47",
   "metadata": {},
   "source": [
    "### CEVAP 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ef8814",
   "metadata": {},
   "source": [
    "Elimizdeki veri setini, train ve validation setleri olarak ayırdığımızda ilk olarak train data seti üzerinde One Hot Encoding işlemi yapılacaktır. Aynı işlem validation data setine yapıldıktan sonra, train ve validation data setleri üzerinde kolon farklılıkları gözlemlenebilir. Bu sebepten dolayı validation data seti oluşturulurken, olmayan kolonla karşılaşma ihtimaline karşın default kolon ataması yapılacak bir kontrol parametresi atanabilir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1072ed20",
   "metadata": {},
   "source": [
    "### SORU 2:\n",
    "\n",
    "Labelencoding'de ilgili kolon için ölçeklendirme nasıl yapılır? (Verinin doğru etkisiyle sayısal olarak dönüştürülmesi)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8219a0",
   "metadata": {},
   "source": [
    "### CEVAP 2: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef916ad",
   "metadata": {},
   "source": [
    "Aşağıdaki oluşturulan gibi bir data frame'imiz olduğunu varsayalım. Modelin üzerinde işlem yapılabilmesi için şehir değişkenlerini Laber Encoding yöntemi ile sayısal verilere döüştürelim. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "652ba675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Kişi</th>\n",
       "      <th>Eğitim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ahmet</td>\n",
       "      <td>Üniversite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mehmet</td>\n",
       "      <td>Lise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Veli</td>\n",
       "      <td>Lise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mustafa</td>\n",
       "      <td>Ortaokul</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Kişi      Eğitim\n",
       "0    Ahmet  Üniversite\n",
       "1   Mehmet        Lise\n",
       "2     Veli        Lise\n",
       "3  Mustafa    Ortaokul"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = {\n",
    "        'Kişi':['Ahmet','Mehmet','Veli','Mustafa'],\n",
    "        'Eğitim':['Üniversite' ,'Lise','Lise','Ortaokul']}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1907bf4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Kişi</th>\n",
       "      <th>Eğitim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ahmet</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mehmet</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Veli</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mustafa</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Kişi  Eğitim\n",
       "0    Ahmet       2\n",
       "1   Mehmet       0\n",
       "2     Veli       0\n",
       "3  Mustafa       1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "lble = LabelEncoder()\n",
    "\n",
    "df.loc[:,\"Eğitim\"] = lble.fit_transform(df.loc[:,\"Eğitim\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ef4d74",
   "metadata": {},
   "source": [
    "Bu aşamada artık eğitim seviyelerimiz için aşağıdaki sayısal ifadelendirmeler kullanılıyor.\n",
    "0 -> Lise, 1 -> Ortaokul, 2->Üniversite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a4df11",
   "metadata": {},
   "source": [
    "### SORU 3:\n",
    "Imbalance datasette train test split yaparken neleri göz önünde bulundurmalıyız?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2872e51d",
   "metadata": {},
   "source": [
    "### CEVAP 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6a32bd",
   "metadata": {},
   "source": [
    "     Öncelikle Imbalace data setin ne anlama geldiğini anlayarak başlayalım.\n",
    "**Imbalance Data Set**: Sınıflandırma yaparken, sınıfların eşit dağılmadığı, yani her sınıf için yaklaşık olarak aynı sayıda verinin olmadığı veri kümesidir. Örnek olarak 500 kişinin katıldığı bir ankette 490 kişinin EVET, 10 kişinin HAYIR cevabını vermesi olabilir. \n",
    "     Imbalace Data Setleri ile karşılaşılan durumlarda algoritmaların güvenilir sonuçlar vermediği durumlarla karşılaşılabilir.\n",
    "Bu durumlarla karşılaşmamak adına, data seti üzerinde train-test split işlemleri yapılırken aşağıdaki yöntemler göz önünde bulundurulabilir.\n",
    "\n",
    "*1)DOĞRU METRİK SEÇİMİ* :\n",
    "\n",
    "Modeller içerisinde çalışan algoritmaların doğruluk oranını ölçmek için kullanılan bir ölçüm tekniğidir. Ancak model performansı için tam olarak doğru ölçüm tekniği değildir.\n",
    "Örneğin aşağıdaki confusion matrix çıktısı incelendiğinde model içerisindeki doğruluk oranı %90'dır. Ancak 2. gruba ait 15 veriden yalnızca 2 si doğru tahmin edilmiştir.\n",
    "\n",
    "Doğruluk :  0.90\n",
    "Hata Matrisi :  \n",
    " [[490  1]\n",
    " [ 10   3]]\n",
    " \n",
    " *2)YENİDEN ÖRNEKLEME (RESAMPLING)* :\n",
    " \n",
    " Imbalanced veri setlerini balanced hale getirmek için bazı resampling teknikleri de kullanılabilir. İki şekilde kullanılabilir.\n",
    " \n",
    "Oversampling  — Azınlık sınıfından örnekleri çoğaltılması.\n",
    "Undersampling — Çoğunluk sınıfından örneklerin azaltılması.\n",
    "\n",
    "\n",
    " *3)KATMANLAMA (STRATIFICATION)* :\n",
    " \n",
    " Katmanlama, verileri, insanları ve nesneleri farklı gruplara veya katmanlara ayırma eylemi olarak tanımlanır. Veriler çok farklı kaynaklardan gelip, çeşitliliğin çok olduğu bir veri seti olduğunda kullanılmalıdır. \n",
    " \n",
    "  *4) SINIF AĞIRLIKLARINI AYARLAMAK (ADJUSTING CLASS WEIGHTS)* :\n",
    "  \n",
    "  Eğer elinizde dengesiz bir veri kümesi varsa, “class_weight” parametresi yardımıyla azınlık sınıfına atanan ağırlığı dengesizlik oranında arttırarak, algoritmanın azınlık verisini yanlış sınıflandırmasından kaynaklanan hata oranını arttırabilirsiniz. Dolayısıyla, modeli oluştururken genel hata oranını azaltmaya çalışan algoritma bu dengesizliği, yani azınlık sınıfı dikkate alacak ve performansı artacaktır.\n",
    "  \n",
    "  *5) PENALIZED MODELLER:*\n",
    "  \n",
    "   Penalized Sınıflandırma yöntemleri (penalized-SVM and penalized-LDA, logistik regresyon vs.) modele, algoritmaları eğitirken, yanlış sınıflandırılan azınlık sınıfı verileri için fazladan cost yüklediği için, modelin azınlık sınıfına verdiği ağırlığı arttırıyor.\n",
    "  \n",
    "  *6) ANOMALİ TESPİTİ (ANOMALY DETECTION)*\n",
    "  \n",
    "   Veri analizinde, anomali tespiti, verilerin çoğunluğundan önemli ölçüde farklılaşarak şüphe uyandıran nadir öğelerin, olayların veya gözlemlerin tanımlanmasıdır."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e834f799",
   "metadata": {},
   "source": [
    "### SORU 4:\n",
    "4-) Validation dataseti (modelin görmediği) nasıl oluşturulur ve nasıl predict etmeye hazır hale getirilir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507843eb",
   "metadata": {},
   "source": [
    "### CEVAP 4:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ede535",
   "metadata": {},
   "source": [
    "**Validation Data Set**: Daha öncesinde oluşturduğumuz train ve test data setlerinden elde ettiğimiz modelimizin hiç görmediği veri setidir. Kendi performansını geliştirmek için kullanacağı kıyaslama verilerini içerir. Yapılan kıyaslamaların yüksek skorlu olması (yani modelin düzgün çalışması) durumunda, modelin güvenilirliği test edilmiş olur. \n",
    "\n",
    "Validation datası oluşturulurken, train data seti üzerinde yapılan preparation işlemleri(encoding, scaling, ..) benzer şekilde uygulanmalı ve validation data seti bu mantığa göre oluşturulmalıdır. Aksi takdirde modelin eğitildiği ve doğrulandığı aşamalar farklı disiplinler içerecek ve doğrulama işlemi sağlıklı olarak yapılamayacaktır. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15a1d0f",
   "metadata": {},
   "source": [
    "### SORU 5:\n",
    " predict_proba metoduyla oran nasıl hesaplanır ve treshold nasıl değiştirilir?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c09d2d",
   "metadata": {},
   "source": [
    "### CEVAP 5:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87eb3df",
   "metadata": {},
   "source": [
    "Sınıflandırma problemlerinde gözlemlerin hangi sınıflara ait olduğunun olasılıklarına ulaşmak istenirse, predict_proba fonksiyonu kullanılır. Fonksiyon kullanımı ve formülü aşağıdaki gibidir.\n",
    "\n",
    "\n",
    "\n",
    "y_pred_proba = dt.predict_proba(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bca7797",
   "metadata": {},
   "source": [
    "### SORU 6:\n",
    " Fraud case'i üzerinde train&test&validation split, encoding, scaling,modelleme çalışmaları Python'da yapılarak, modelin görmediği dataset üzerinde başarılı sonuç alacak bir model örneği yapılmalı.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1798c54",
   "metadata": {},
   "source": [
    "### CEVAP 6:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e75ace",
   "metadata": {},
   "source": [
    "***-Kütüphanelerin Import Edilme İşlemi***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "083699d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plotly'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-08402376f662>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpress\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph_objects\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mgo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'plotly'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "#for quick viz\n",
    "import seaborn as sns\n",
    "\n",
    "#ml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83661cf6",
   "metadata": {},
   "source": [
    "***-Veri Ön İşleme Aşamaları***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac06ce87",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'auto_insurance_csv.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-0f18cc011cbe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'auto_insurance_csv.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'_c39'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Unnamed: 0'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'umbrella_limit'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mumbrella_limit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"9999\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1048\u001b[0m             )\n\u001b[0;32m   1049\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1050\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1051\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1866\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1867\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1868\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1869\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"encoding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m         \"\"\"\n\u001b[1;32m-> 1362\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"replace\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 642\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'auto_insurance_csv.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('auto_insurance_csv.csv')\n",
    "df = data.copy()\n",
    "\n",
    "df = df.drop(['_c39','Unnamed: 0'], axis = 1)\n",
    "df['umbrella_limit'] = df.umbrella_limit.fillna(\"9999\")\n",
    "df['police_report_available'] = df.police_report_available.fillna(\"MISSING\")\n",
    "df['policy_csl'] = df.policy_csl.fillna(\"MISSING\")\n",
    "df['policy_bind_date'] = pd.to_datetime(df['policy_bind_date'])\n",
    "df['incident_date'] = pd.to_datetime(df['incident_date'])\n",
    "df['claim_day_of_policy'] = (df.incident_date -  df.policy_bind_date).dt.days\n",
    "df['location_check'] = np.nan\n",
    "df['location_check'] = np.where(df['policy_state'] == df['incident_state'], True, False)\n",
    "df['fraud_reported'] = df['fraud_reported'].str.replace('Y', '1')\n",
    "df['fraud_reported'] = df['fraud_reported'].str.replace('N', '0')\n",
    "df['fraud_reported'] = df['fraud_reported'].astype(int)\n",
    "df['umbrella_limit'] = df.umbrella_limit.astype(str)\n",
    "umbrealla = df['umbrella_limit'].unique()\n",
    "for umb in umbrealla:\n",
    "  if (umb != '0.0') & (umb != '9999'):\n",
    "    df['umbrella_limit'] = df['umbrella_limit'].str.replace(umb, 'other')\n",
    "    \n",
    "hobbies = df['insured_hobbies'].unique()\n",
    "for hobby in hobbies:\n",
    "  if (hobby != 'chess') & (hobby != 'cross-fit'):\n",
    "    df['insured_hobbies'] = df['insured_hobbies'].str.replace(hobby, 'other')\n",
    "\n",
    "df['age'] = df.age.fillna(9999)\n",
    "bin_labels = ['15-20', '21-25', '26-30', '31-35', '36-40', '41-45', '46-50', '51-55', '56-60', '61-65','9999']\n",
    "bins = [15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 9999]\n",
    "\n",
    "df['age_group'] = pd.cut(df['age'], bins = bins, labels = bin_labels, include_lowest = True)\n",
    "bins = [0, 50, 100, 150, 200, 250, 300, 350, 400, 450, 500]\n",
    "bin_labels = ['0-50','51-100','101-150','151-200','201-250','251-300','301-350','351-400','401-450','451-500']\n",
    "\n",
    "df['months_as_customer_groups'] = pd.cut(df['months_as_customer'], bins = 10, labels = bin_labels, include_lowest= True)\n",
    "bins = list(np.linspace(0,2500, 6, dtype = int))\n",
    "bin_labels = ['very low', 'low', 'medium', 'high', 'very high']\n",
    "\n",
    "df['policy_annual_premium_groups'] = pd.cut(df['policy_annual_premium'], bins = bins, labels=bin_labels)\n",
    "bins = list(np.linspace(0,2000, 5, dtype = int))\n",
    "bin_labels = ['0-500', '501-1000', '1001-1500', '1501-2000']\n",
    "\n",
    "df['policy_deductable_group'] = pd.cut(df['policy_deductable'], bins = bins, labels = bin_labels)\n",
    "\n",
    "df = df.drop(['age', 'months_as_customer', 'policy_deductable', 'policy_annual_premium'], axis = 1)\n",
    "required_columns = ['incident_date','policy_state', 'policy_csl', 'umbrella_limit',\n",
    "       'insured_zip', 'insured_sex', 'insured_education_level',\n",
    "       'insured_occupation', 'insured_hobbies', 'insured_relationship',\n",
    "       'capital-gains', 'capital-loss', 'incident_type', 'collision_type',\n",
    "       'incident_severity', 'authorities_contacted', 'incident_state',\n",
    "       'incident_city', 'incident_location', 'incident_hour_of_the_day',\n",
    "       'number_of_vehicles_involved', 'property_damage', 'bodily_injuries',\n",
    "       'witnesses', 'police_report_available', 'total_claim_amount','auto_make',\n",
    "       'auto_model', 'auto_year', 'fraud_reported', 'claim_day_of_policy',\n",
    "       'location_check', 'age_group', 'months_as_customer_groups',\n",
    "       'policy_annual_premium_groups', 'policy_deductable_group']\n",
    "\n",
    "df1 = df[required_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddf0f454",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-ea4a4376cc7b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Elimizdeki Veri setini 200 validation, 800 train data seti olarak ayırıyoruz.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdf1_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'incident_date'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mascending\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mdf2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'incident_date'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mascending\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtail\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m800\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df1' is not defined"
     ]
    }
   ],
   "source": [
    "#Elimizdeki Veri setini 200 validation, 800 train data seti olarak ayırıyoruz. \n",
    "\n",
    "df1_val = df1.sort_values(by='incident_date',ascending=False).head(200)\n",
    "df2 = df1.sort_values(by='incident_date',ascending=False).tail(800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48de5ac3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df1_val' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-ac3dd9835b32>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#En başında, verilerimizi \"incident date\" kolonuna göre tarihsel olarak sıraladığımız için bu kolonu droplayabiliriz.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf1_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf1_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"incident_date\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdf2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"incident_date\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df1_val' is not defined"
     ]
    }
   ],
   "source": [
    "#En başında, verilerimizi \"incident date\" kolonuna göre tarihsel olarak sıraladığımız için bu kolonu droplayabiliriz.\n",
    "df1_val = df1_val.drop([\"incident_date\"],axis=1)\n",
    "df2 = df2.drop([\"incident_date\"],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb151913",
   "metadata": {},
   "source": [
    "***-Encoding İşlemleri***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a20fda85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Öncelikle encoding işlemi yapılacak olan kolonlara karar verilip, veri türlerini kategorik olarak değiştiriyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf08aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['age_group', 'months_as_customer_groups', 'policy_annual_premium_groups','location_check','policy_deductable_group']\n",
    "for col in cat_cols:\n",
    "  df2[col] = df2[col].astype('object')\n",
    "\n",
    "columns_to_encode = []\n",
    "for col in df2.columns:\n",
    "  if df2[col].dtype == 'object':\n",
    "    columns_to_encode.append(col)\n",
    "\n",
    "columns_to_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42ff2601",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-e2c3d2996791>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# OneHotEncoding işlemi yapıyoruz.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mohe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolumns_to_encode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mohe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df2' is not defined"
     ]
    }
   ],
   "source": [
    "# OneHotEncoding işlemi yapıyoruz.\n",
    "\n",
    "ohe = pd.get_dummies(df2, columns = columns_to_encode)\n",
    "\n",
    "ohe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b1b9ce3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ohe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-fc46ea9398a9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mohe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mnum_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mohe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_numeric_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_cols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ohe' is not defined"
     ]
    }
   ],
   "source": [
    "cols = ohe.columns\n",
    "num_cols = ohe._get_numeric_data().columns\n",
    "list(set(cols) - set(num_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8a2e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "for col in df3.columns:\n",
    "  if col != 'fraud_reported':\n",
    "    features.append(col)\n",
    "\n",
    "target = 'fraud_reported'\n",
    "\n",
    "X = df3[features]\n",
    "y = df3[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8406b8",
   "metadata": {},
   "source": [
    "***-Scale İşlemi***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f7e35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1d286b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
